@software{AddonItem,
  title = {Addon {{Item}}}
}

@online{carionEndtoEndObjectDetection2020,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  date = {2020-05-28},
  eprint = {2005.12872},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.12872},
  url = {http://arxiv.org/abs/2005.12872},
  urldate = {2024-04-20},
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Todo},
  file = {/Users/azurice/Zotero/storage/7ACQS5B9/Carion 等 - 2020 - End-to-End Object Detection with Transformers.pdf;/Users/azurice/Zotero/storage/WWT57DLT/2005.html}
}

@article{felzenszwalbEfficientGraphBasedImage2004,
  title = {Efficient {{Graph-Based Image Segmentation}}},
  author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
  date = {2004-09},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {International Journal of Computer Vision},
  volume = {59},
  number = {2},
  pages = {167--181},
  issn = {0920-5691},
  doi = {10.1023/B:VISI.0000022288.19776.77},
  url = {http://link.springer.com/10.1023/B:VISI.0000022288.19776.77},
  urldate = {2024-04-16},
  abstract = {This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.},
  langid = {english},
  keywords = {Todo},
  file = {/Users/azurice/Zotero/storage/7IQGRXNF/Felzenszwalb和Huttenlocher - 2004 - Efficient Graph-Based Image Segmentation.pdf}
}

@article{felzenszwalbEfficientGraphBasedImage2004a,
  title = {Efficient {{Graph-Based Image Segmentation}}},
  author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
  date = {2004-09-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {International Journal of Computer Vision},
  volume = {59},
  number = {2},
  pages = {167--181},
  issn = {1573-1405},
  doi = {10.1023/B:VISI.0000022288.19776.77},
  url = {https://doi.org/10.1023/B:VISI.0000022288.19776.77},
  urldate = {2024-04-16},
  abstract = {This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.},
  langid = {english},
  keywords = {clustering,Finished,graph algorithm,image segmentation,perceptual organization},
  file = {/Users/azurice/Zotero/storage/H3XSGRUR/Felzenszwalb和Huttenlocher - 2004 - Efficient Graph-Based Image Segmentation.pdf}
}

@inproceedings{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Girshick, Ross},
  date = {2015-12},
  pages = {1440--1448},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2015.169},
  url = {https://ieeexplore.ieee.org/document/7410526},
  urldate = {2024-04-16},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {Computer architecture,Feature extraction,Finished,Object detection,Open source software,Pipelines,Proposals,Training},
  file = {/Users/azurice/Zotero/storage/6ESBD3A6/Girshick - 2015 - Fast R-CNN.pdf;/Users/azurice/Zotero/storage/XKTILARH/7410526.html}
}

@inproceedings{girshickRichFeatureHierarchies2014a,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014},
  pages = {580--587},
  url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
  urldate = {2024-04-15},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Todo},
  file = {/Users/azurice/Zotero/storage/2H39YPMS/Girshick 等 - 2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf}
}

@online{girshickRichFeatureHierarchies2014b,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-10-22},
  eprint = {1311.2524},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1311.2524},
  url = {http://arxiv.org/abs/1311.2524},
  urldate = {2024-04-15},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/\textasciitilde rbg/rcnn.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Todo},
  file = {/Users/azurice/Zotero/storage/FDZIE4KY/Girshick 等 - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf;/Users/azurice/Zotero/storage/ZBMT3PE2/1311.html}
}

@inproceedings{girshickRichFeatureHierarchies2014c,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-06},
  pages = {580--587},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.81},
  url = {https://ieeexplore.ieee.org/document/6909475},
  urldate = {2024-04-15},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 – achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Feature extraction,Object detection,Proposals,Support vector machines,Todo,Training,Vectors,Visualization},
  file = {/Users/azurice/Zotero/storage/VPJFSB2N/Girshick 等 - 2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf;/Users/azurice/Zotero/storage/HZWLISM9/6909475.html}
}

@inproceedings{girshickRichFeatureHierarchies2014d,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-06},
  pages = {580--587},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.81},
  url = {https://ieeexplore.ieee.org/document/6909475},
  urldate = {2024-04-15},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 – achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Feature extraction,Finished,Object detection,Proposals,Support vector machines,Training,Vectors,Visualization},
  file = {/Users/azurice/Zotero/storage/2HUHQ95V/Girshick 等 - 2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf;/Users/azurice/Zotero/storage/34G9T46F/6909475.html}
}

@inproceedings{heSpatialPyramidPooling2014,
  title = {Spatial {{Pyramid Pooling}} in {{Deep Convolutional Networks}} for {{Visual Recognition}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2014},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  date = {2014},
  pages = {346--361},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-10578-9_23},
  abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g.~224×224) input image. This requirement is “artificial” and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101.},
  isbn = {978-3-319-10578-9},
  langid = {english},
  keywords = {Todo},
  file = {/Users/azurice/Zotero/storage/P7SBBTRT/He 等 - 2014 - Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  url = {https://dl.acm.org/doi/10.1145/3065386},
  urldate = {2024-04-15},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  keywords = {Todo},
  file = {/Users/azurice/Zotero/storage/WS3DXWRE/Krizhevsky 等 - 2017 - ImageNet classification with deep convolutional neural networks.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017a,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782},
  doi = {10.1145/3065386},
  url = {https://dl.acm.org/doi/10.1145/3065386},
  urldate = {2024-04-15},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  keywords = {Todo},
  file = {/Users/azurice/Zotero/storage/4C74AV22/Krizhevsky 等 - 2017 - ImageNet classification with deep convolutional neural networks.pdf}
}

@online{lawCornerNetDetectingObjects2019,
  title = {{{CornerNet}}: {{Detecting Objects}} as {{Paired Keypoints}}},
  shorttitle = {{{CornerNet}}},
  author = {Law, Hei and Deng, Jia},
  date = {2019-03-18},
  eprint = {1808.01244},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1808.01244},
  url = {http://arxiv.org/abs/1808.01244},
  urldate = {2024-04-20},
  abstract = {We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2\% AP on MS COCO, outperforming all existing one-stage detectors.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Todo},
  file = {/Users/azurice/Zotero/storage/XZ2RGEFX/Law和Deng - 2019 - CornerNet Detecting Objects as Paired Keypoints.pdf;/Users/azurice/Zotero/storage/MQX3DWAI/1808.html}
}

@article{liangSystematicReviewImagelevel2024,
  title = {A Systematic Review of Image-Level Camouflaged Object Detection with Deep Learning},
  author = {Liang, Yanhua and Qin, Guihe and Sun, Minghui and Wang, Xinchao and Yan, Jie and Zhang, Zhonghan},
  date = {2024-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {566},
  pages = {127050},
  issn = {09252312},
  doi = {10.1016/j.neucom.2023.127050},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231223011736},
  urldate = {2024-04-20},
  abstract = {Camouflaged object detection (COD) aims to search and identify disguised objects that are hidden in their surrounding environment, thereby deceiving the human visual system. As an interesting and challenging task, COD has received increasing attention from the community in the past few years, especially for image-level camouflaged object segmentation task. So far, some advanced image-level COD models have been proposed, mainly dominated by deep learning-based solutions. To have an in-depth understanding of existing image-level COD methods in the deep learning era, in this paper, we give a comprehensive review on model structure and paradigm classification, public benchmark datasets, evaluation metrics, model performance benchmark, and potential future development directions. Specifically, we first review 96 existing deep COD algorithms. Subsequently, we summarize and analyze the existing five widely used COD datasets and evaluation metrics. Furthermore, we benchmark a set of representative models and provide a detailed analysis of the comparison results from both quantitative and qualitative perspectives. Moreover, we further discuss the challenges of COD and the corresponding solutions. Finally, based on the understanding of this field, future development trends and potential research directions are prospected. In conclusion, the purpose of this paper is to provide researchers with a review of the latest COD methods, increase their understanding of COD research, and gain some enlightenment.},
  langid = {english},
  keywords = {Todo},
  file = {/Users/azurice/Zotero/storage/VJJ7KIGK/Liang 等 - 2024 - A systematic review of image-level camouflaged object detection with deep learning.pdf}
}

@online{linFeaturePyramidNetworks2017,
  title = {Feature {{Pyramid Networks}} for {{Object Detection}}},
  author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  date = {2017-04-19},
  eprint = {1612.03144},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1612.03144},
  url = {http://arxiv.org/abs/1612.03144},
  urldate = {2024-04-16},
  abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Finished,Object Detection/Two-Stage},
  file = {/Users/azurice/Zotero/storage/9NL2ZVAH/Lin 等 - 2017 - Feature Pyramid Networks for Object Detection.pdf;/Users/azurice/Zotero/storage/BFLUIVMT/1612.html}
}

@article{linFocalLossDense2020,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  date = {2020-02},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {42},
  number = {2},
  pages = {318--327},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2858826},
  url = {https://ieeexplore.ieee.org/document/8417976},
  urldate = {2024-04-20},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Computer vision,convolutional neural networks,Convolutional neural networks,Detectors,Entropy,Feature extraction,machine learning,object detection,Object detection,Proposals,Todo,Training},
  file = {/Users/azurice/Zotero/storage/L5GJT79K/Lin 等 - 2020 - Focal Loss for Dense Object Detection.pdf;/Users/azurice/Zotero/storage/AESGPMVF/8417976.html}
}

@inproceedings{liuSSDSingleShot2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2016},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  date = {2016},
  pages = {21--37},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-46448-0_2},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For \$\$300 \textbackslash times 300\$\$300×300input, SSD achieves 74.3~\% mAP on VOC2007 test at 59~FPS on a Nvidia Titan X and for \$\$512 \textbackslash times 512\$\$512×512input, SSD achieves 76.9~\% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
  isbn = {978-3-319-46448-0},
  langid = {english},
  keywords = {Todo},
  file = {/Users/azurice/Zotero/storage/3C84MVIT/Liu 等 - 2016 - SSD Single Shot MultiBox Detector.pdf}
}

@article{liuSurveyPerformanceEvaluation2021a,
  title = {A Survey and Performance Evaluation of Deep Learning Methods for Small Object Detection},
  author = {Liu, Yang and Sun, Peng and Wergeles, Nickolas and Shang, Yi},
  date = {2021-06},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {172},
  pages = {114602},
  issn = {09574174},
  doi = {10.1016/j.eswa.2021.114602},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421000439},
  urldate = {2024-04-20},
  abstract = {In computer vision, significant advances have been made on object detection with the rapid development of deep convolutional neural networks (CNN). This paper provides a comprehensive review of recently developed deep learning methods for small object detection. We summarize challenges and solutions of small object detection, and present major deep learning techniques, including fusing feature maps, adding context information, balancing foreground-background examples, and creating sufficient positive examples. We discuss related techniques developed in four research areas, including generic object detection, face detection, object detection in aerial imagery, and segmentation. In addition, this paper compares the performances of several leading deep learning methods for small object detection, including YOLOv3, Faster R-CNN, and SSD, based on three large benchmark datasets of small objects. Our experimental results show that while the detection accuracy on small objects by these deep learning methods was low, less than 0.4, Faster R-CNN performed the best, while YOLOv3 was a close second.},
  langid = {english},
  keywords = {Todo},
  file = {/Users/azurice/Zotero/storage/YZP7RPD7/Liu 等 - 2021 - A survey and performance evaluation of deep learning methods for small object detection.pdf}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-06},
  pages = {779--788},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.91},
  url = {https://ieeexplore.ieee.org/document/7780460},
  urldate = {2024-04-16},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Computer architecture,Microprocessors,Neural networks,Object detection,Pipelines,Real-time systems,Todo,Training},
  file = {/Users/azurice/Zotero/storage/8CEN9Z9G/Redmon 等 - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf;/Users/azurice/Zotero/storage/YY4VV9MD/7780460.html}
}

@online{renFasterRCNNRealTime2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2016-01-06},
  eprint = {1506.01497},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.01497},
  url = {http://arxiv.org/abs/1506.01497},
  urldate = {2024-04-16},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Finished,Object Detection/Two-Stage},
  file = {/Users/azurice/Zotero/storage/6L3VYBJ4/Ren 等 - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf;/Users/azurice/Zotero/storage/JNBKKNVU/1506.html}
}

@online{RichFeatureHierarchiesb,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/6909475},
  urldate = {2024-04-15},
  keywords = {Todo},
  file = {/Users/azurice/Zotero/storage/EI9QCVTP/6909475.html}
}

@online{sermanetPedestrianDetectionUnsupervised2013,
  title = {Pedestrian {{Detection}} with {{Unsupervised Multi-Stage Feature Learning}}},
  author = {Sermanet, Pierre and Kavukcuoglu, Koray and Chintala, Soumith and LeCun, Yann},
  date = {2013-04-02},
  eprint = {1212.0142},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1212.0142},
  url = {http://arxiv.org/abs/1212.0142},
  urldate = {2024-04-15},
  abstract = {Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Todo},
  file = {/Users/azurice/Zotero/storage/WCUVBSJP/Sermanet 等 - 2013 - Pedestrian Detection with Unsupervised Multi-Stage Feature Learning.pdf;/Users/azurice/Zotero/storage/KGP5YH3X/1212.html}
}

@article{uijlingsSelectiveSearchObject2013a,
  title = {Selective {{Search}} for {{Object Recognition}}},
  author = {Uijlings, J. R. R. and family=Sande, given=K. E. A., prefix=van de, useprefix=true and Gevers, T. and Smeulders, A. W. M.},
  date = {2013-09-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {104},
  number = {2},
  pages = {154--171},
  issn = {1573-1405},
  doi = {10.1007/s11263-013-0620-5},
  url = {https://doi.org/10.1007/s11263-013-0620-5},
  urldate = {2024-04-16},
  abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99~\% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/\textasciitilde uijlings/SelectiveSearch.html).},
  langid = {english},
  keywords = {Appearance Model,Colour Space,Exhaustive Search,Finished,Object Location,Object Recognition},
  file = {/Users/azurice/Zotero/storage/6I583XGZ/Uijlings 等 - 2013 - Selective Search for Object Recognition.pdf}
}

@online{zhouObjectsPoints2019,
  title = {Objects as {{Points}}},
  author = {Zhou, Xingyi and Wang, Dequan and Krähenbühl, Philipp},
  date = {2019-04-25},
  eprint = {1904.07850},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.07850},
  url = {http://arxiv.org/abs/1904.07850},
  urldate = {2024-04-20},
  abstract = {Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Todo},
  file = {/Users/azurice/Zotero/storage/X2WZHG44/Zhou 等 - 2019 - Objects as Points.pdf;/Users/azurice/Zotero/storage/TGYR48K6/1904.html}
}

@online{zhuDeformableDETRDeformable2021,
  title = {Deformable {{DETR}}: {{Deformable Transformers}} for {{End-to-End Object Detection}}},
  shorttitle = {Deformable {{DETR}}},
  author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  date = {2021-03-17},
  eprint = {2010.04159},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.04159},
  url = {http://arxiv.org/abs/2010.04159},
  urldate = {2024-04-20},
  abstract = {DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Todo},
  file = {/Users/azurice/Zotero/storage/SXX4YWX6/Zhu 等 - 2021 - Deformable DETR Deformable Transformers for End-to-End Object Detection.pdf;/Users/azurice/Zotero/storage/P5J7LQBW/2010.html}
}

@article{zouObjectDetection202023,
  title = {Object {{Detection}} in 20 {{Years}}: {{A Survey}}},
  shorttitle = {Object {{Detection}} in 20 {{Years}}},
  author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
  date = {2023-03},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {111},
  number = {3},
  pages = {257--276},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2023.3238524},
  url = {https://ieeexplore.ieee.org/document/10028728/},
  urldate = {2024-04-09},
  file = {/Users/azurice/Zotero/storage/76L7B824/Zou 等 - 2023 - Object Detection in 20 Years A Survey.pdf}
}

@article{zouObjectDetection202023a,
  title = {Object {{Detection}} in 20 {{Years}}: {{A Survey}}},
  shorttitle = {Object {{Detection}} in 20 {{Years}}},
  author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
  date = {2023-03},
  journaltitle = {Proceedings of the IEEE},
  volume = {111},
  number = {3},
  pages = {257--276},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2023.3238524},
  url = {https://ieeexplore.ieee.org/abstract/document/10028728},
  urldate = {2024-04-09},
  abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today’s object detection technique as a revolution driven by deep learning, then, back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This article extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century’s time (from the 1990s to 2022). A number of topics have been covered in this article, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speedup techniques, and recent state-of-the-art detection methods.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Computer vision,Convolutional neural networks,convolutional neural networks (CNNs),deep learning,Deep learning,Detectors,Feature extraction,In Progress,object detection,Object detection,technical evolution},
  file = {/Users/azurice/Zotero/storage/5T6XDT44/Zou 等 - 2023 - Object Detection in 20 Years A Survey.pdf;/Users/azurice/Zotero/storage/FYHFLPUT/10028728.html}
}
